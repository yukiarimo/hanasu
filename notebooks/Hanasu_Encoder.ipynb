{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Hanasu Encoder Trainer"
      ],
      "metadata": {
        "id": "gcxFklhazhBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "mj9FmXbqqQIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model"
      ],
      "metadata": {
        "id": "71SMwsDBztEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForMaskedLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "ldFgWJ0qpwSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify your model checkpoint\n",
        "model_checkpoint = \"yukiarimo/yuna-ai-hanasu-v1\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "B4Th4CoyzSKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load your dataset"
      ],
      "metadata": {
        "id": "EVp7v-CFz0wi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPEeu1CupoOk"
      },
      "outputs": [],
      "source": [
        "# raw text file\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": \"/content/drive/MyDrive/dataset.txt\"})\n",
        "\n",
        "# We define a block size for training (adjust as needed; note that mDeBERTa was pre-trained with a fixed max length)\n",
        "block_size = 512\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize each example and truncate to block_size.\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=block_size)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Optionally, to create contiguous blocks over the entire text (instead of individual lines), we can concatenate and re-split:\n",
        "def group_texts(examples):\n",
        "    # Concatenate all token lists.\n",
        "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated[list(examples.keys())[0]])\n",
        "    # Drop the small remainder\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated.items()\n",
        "    }\n",
        "    return result\n",
        "\n",
        "# Group tokens into blocks for efficient training.\n",
        "lm_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
        "\n",
        "# Create a data collator that will dynamically mask tokens for the MLM objective.\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is where magic happens"
      ],
      "metadata": {
        "id": "MmEtyzgz0b_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mdeberta-finetuned-light-novels\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,                    # Adjust epochs as needed\n",
        "    per_device_train_batch_size=4,         # Adjust batch size based on your GPU memory. 8 = 16GB GPU\n",
        "    save_steps=1,\n",
        "    learning_rate=1e-5,                     # Adjust learning rate as needed\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,                              # Use mixed precision training\n",
        "    save_strategy=\"epoch\",\n",
        "    prediction_loss_only=True,             # Only compute the loss for the masked language model. Thhs is important for efficiency.\n",
        "    logging_steps=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Initialize the Trainer.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start fine-tuning.\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "z45X1fDCps9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save in safetensors"
      ],
      "metadata": {
        "id": "uzWeS9hV0g8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint_path = \"/content/checkpoint\"\n",
        "final_model_path = \"/content/hanasu-v1\"\n",
        "\n",
        "# Load the model from the checkpoint\n",
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "\n",
        "# Save as the final model\n",
        "model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "print(f\"Final model saved to {final_model_path}\")"
      ],
      "metadata": {
        "id": "9X1ru2HB02wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "LsZQC2n_0sDK"
      }
    },
    {
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "model_path = \"/content/hanasu-v1\"  # Path to your saved model\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VGZU2x714O09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "\n",
        "input_ids = tokenizer.encode(\"こんにちは、[MASK]元気ですか？\", return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    predictions = outputs.logits  # Get the model's predictions\n",
        "\n",
        "masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1]  # Find the positions of the masked tokens\n",
        "predicted_token_ids = torch.argmax(predictions[0, masked_index], dim=-1)  # Get the predicted token IDs for masked positions\n",
        "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)  # Convert IDs back to tokens\n",
        "\n",
        "# reconstruct the sentence\n",
        "reconstructed_sentence = tokenizer.decode(input_ids[0])\n",
        "for token in predicted_tokens:\n",
        "    reconstructed_sentence = reconstructed_sentence.replace(tokenizer.mask_token, token, 1)\n",
        "\n",
        "print(reconstructed_sentence)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zXPd6WR54Pj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}